# -*- coding: utf-8 -*-
"""CarOwnersNationwide.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WvfjcX2--iU-LhpJG0gxOGoUXkXws8Qn
"""

!pip install googletrans==4.0.0-rc1

import pandas as pd
import re
from googletrans import Translator

# Load the dataset from the CSV file
df = pd.read_csv('760k-Car-Owners-Nationwide-China-csv-2020.csv', low_memory=False)

# Display the first few rows of the dataset to understand its content
print("The first few rows of the dataset:")
print(df.head())

# Get basic information about the dataset
print("\nBasic information about the dataset:")
print(df.info())

# prompt: How do I translate the column headings using a translation library?

# Initialize the translator
translator = Translator()

# Translate the column headings
new_column_names = []
for col in df.columns:
  try:
    translated_col = translator.translate(col, dest='en').text
    new_column_names.append(translated_col)
  except:
    new_column_names.append(col)  # Keep original if translation fails


# Assign the new column names to the dataframe
df.columns = new_column_names

# Display the first few rows of the dataset with translated column headings
print("The first few rows of the dataset with translated column headings:")
print(df.head())
# Get basic information about the dataset
print("\nBasic information about the dataset:")
print(df.info())

# Normalize the columns to lowercase and replace all spaces with underscores

def normalize_columns(df):
  new_columns = []
  for column in df.columns:
    new_column = column.lower().replace(' ', '_')
    new_columns.append(new_column)
  df.columns = new_columns
  return df

df = normalize_columns(df)

print(df.info())

# prompt: How do I drop one or more columns from the dataframe?

# Assuming you want to drop columns 'Monthly salary', 'educate', etc.
df = df.drop(['frame_number', 'monthly_salary', 'educate', 'brand', 'car', 'model', 'configuration', 'color', 'motor_number', 'unnamed:_21'], axis=1)

# Display the updated dataframe
print(df.info())

# Check for missing values
print(df.isnull().sum())

# Fill in the missing post code values.
df['post_code'] = df['post_code'].fillna(0.0)

df.head()

# Convert 'post code' values from floating point to integer numbers and then to
# strings.
df['post_code'] = df['post_code'].astype(int).astype(str)

df.head()

# prompt: Please fill all missing values with the string 'NA'.

# Fill missing values with 'NA'
df.fillna('NA', inplace=True)

# Re-check for missing values
print(df.isnull().sum())

# Identify duplicates (excluding the first occurrence)
duplicates = df[df.duplicated(subset=['id_card'], keep='first')]

# Export duplicate rows to a separate CSV file
duplicates.to_csv('duplicate_id_cards.csv', index=False)

# Drop duplicates from the original DataFrame, keeping the first occurrence
df_unique = df.drop_duplicates(subset=['id_card'], keep='first')

df_unique.info()

# prompt: Please check for invalid e-mail addresses, remove those rows, and export them to a separate .csv file.

# Define a regular expression pattern for valid email addresses
email_pattern = r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"

# Find rows with invalid email addresses
invalid_emails_df = df_unique[~df_unique['mail'].str.match(email_pattern, na=False)]

# Remove rows with invalid email addresses from the original DataFrame
df_valid_emails = df_unique[df_unique['mail'].str.match(email_pattern, na=False)]

# Export the rows with invalid emails to a separate CSV file
invalid_emails_df.to_csv('rows_with_invalid_emails.csv', index=False)

# Reset the index
df_indexed = df_valid_emails.reset_index(drop=True)

df_indexed.tail()

# Export the cleaned datafame to a new CSV file named 'cleaned_data.csv'
df_indexed.to_csv('cleaned_data.csv')

# prompt: Please merge the 'duplicate_id_cards.csv'and 'rows_with_invalid_emails.csv' into one file named 'garbage.csv'.

# Load the two CSV files into DataFrames
duplicate_df = pd.read_csv('duplicate_id_cards.csv')
invalid_emails_df = pd.read_csv('rows_with_invalid_emails.csv')

# Concatenate the two DataFrames into one
garbage_df = pd.concat([duplicate_df, invalid_emails_df], ignore_index=True)

# Export the merged DataFrame to a new CSV file named 'garbage.csv'
garbage_df.to_csv('garbage.csv', index=False)

garbage_df.tail()

df_indexed.info()

