# -*- coding: utf-8 -*-
"""Lifebear.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ds6vSNXbd9Om_zDZehLMrugQ6LtwuhL3
"""

!pip install boto3

!pip install mysql-connector-python

import boto3
import psycopg2

import pandas as pd
import numpy as np
# from dotenv import load_env
import os
import csv
import re

import json
import random
from datetime import datetime
from dateutil import parser
import matplotlib.pyplot as plt
import math
import mysql.connector

file_path = "3.6M-Japan-lifebear.com-Largest-Notebook-App-UsersDB-csv-2019.csv"

lifebear_dataset = pd.read_csv(file_path, sep=';', low_memory=False)
lifebear_dataset

print(lifebear_dataset.isnull().sum())

lifebear_dataset["login_id"] = lifebear_dataset["login_id"].fillna("NA")

lifebear_dataset["gender"] = lifebear_dataset["gender"].fillna(-1.0)

lifebear_dataset["birthday_on"] = lifebear_dataset["birthday_on"].fillna("NA")

lifebear_dataset

#Drop rows with duplicate values that exist in both "login_id" and "mail_address" columns

# Identify duplicate rows based on "login_id" and "mail_address"
lifebear_duplicates = lifebear_dataset[lifebear_dataset.duplicated(subset=["login_id", "mail_address"], keep=False)]

# Save the duplicate rows to a garbage file
lifebear_duplicates.to_csv("lifebear_dup_garbage.csv", index=False)

# Drop the duplicate rows from the original dataframe
lifebear_dataset = lifebear_dataset.drop_duplicates(subset=["login_id", "mail_address"], keep='first')

# prompt: How do I remove the time part from the "created_at" column?

# Assuming "created_at" column contains datetime values
lifebear_dataset['created_at'] = pd.to_datetime(lifebear_dataset['created_at']).dt.date

lifebear_dataset.head(5)

# Rename a specific column
lifebear_dataset.rename(columns={'mail_address': 'e_mail_address','created_at': 'created_on','birthday_on': 'birthdate'}, inplace=True)

lifebear_dataset.head(5)

# prompt: Please check for invalid e-mail addresses, remove those rows, and export them to a separate .csv file.

def is_valid_email(email):
  """Check if an email address is valid using a regular expression."""
  pattern = r"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}$"
  return bool(re.match(pattern, email))

# Find rows with invalid email addresses
invalid_emails_df = lifebear_dataset[~lifebear_dataset['e_mail_address'].apply(is_valid_email)]

# Remove rows with invalid emails from the original DataFrame
lifebear_dataset = lifebear_dataset[lifebear_dataset['e_mail_address'].apply(is_valid_email)]

# Export the invalid email rows to a CSV file
invalid_emails_df.to_csv('invalid_email_addresses.csv', index=False)

print("Invalid email addresses have been exported to invalid_email_addresses.csv")

# prompt: Merge the 'lifebear_dup_garbage.csv' and 'invalid_email_addresses.csv' files into one 'garbage.csv' file.

# Load the two CSV files into pandas DataFrames
lifebear_dup_garbage_df = pd.read_csv('lifebear_dup_garbage.csv')
invalid_email_addresses_df = pd.read_csv('invalid_email_addresses.csv')

# Concatenate the two DataFrames into a single DataFrame
garbage_df = pd.concat([lifebear_dup_garbage_df, invalid_email_addresses_df], ignore_index=True)

# Save the merged DataFrame to a new CSV file named 'garbage.csv'
garbage_df.to_csv('garbage.csv', index=False)

print("The two files have been merged into 'garbage.csv'")

lifebear_dataset = lifebear_dataset.reset_index(drop=True)

lifebear_dataset.tail(5)

# Convert all text to lowercase
lifebear_dataset['login_id'] = lifebear_dataset['login_id'].str.lower()

# Strip leading and trailing whitespace
lifebear_dataset['login_id'] = lifebear_dataset['login_id'].str.strip()
lifebear_dataset['password'] = lifebear_dataset['password'].str.strip()
lifebear_dataset['salt'] = lifebear_dataset['salt'].str.strip()

# prompt: Output the cleaned dataframe to a .csv file.

lifebear_dataset.to_csv('cleaned_lifebear_dataset.csv', index=False)