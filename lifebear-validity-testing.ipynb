{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab81eb32-00de-4874-b4cd-d78739d6c7ae",
   "metadata": {},
   "source": [
    "# Base Declaration of Project Libraries\n",
    "> To ensure effective data cleaning & ingestion; reduce the size of the entire dataset and execute standard formatting processes on each file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb5ab02-4d1b-42ea-b86a-1500cb026e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import psycopg2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import simplejson as json\n",
    "import random\n",
    "from datetime import datetime\n",
    "from dateutil import parser\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad251fa8-db4c-41a2-937c-b2dd4c4ab5bb",
   "metadata": {},
   "source": [
    "### Initial Review & Assessment of the raw data file\n",
    "> Note the separating character for this dataset is a semi-colon & not the traditional comma character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1383f3d-07e9-4065-bd55-e0877eb60f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\".//datasets//ingested//lifebear//lifebear.csv\"\n",
    "\n",
    "lifebear_dataset = pd.read_csv(file_path, sep=';', low_memory=True)\n",
    "lifebear_dataset.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c835a2-7f1e-4299-a3c9-0d35b3047fc2",
   "metadata": {},
   "source": [
    "### Dataset Chunking\n",
    "> Breaking a large file into multiple smaller files increases the probability of data ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099772b1-d3f9-464a-ba19-2b21a4cfa3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to the folder containing the files\n",
    "file_path = f\".//datasets//ingested//lifebear//lifebear.csv\"\n",
    "output_folder = f\".//datasets//ingested//lifebear//records//chunks//\"\n",
    "\n",
    "def split_file(file_path, chunk_size, output_directory):\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    \n",
    "    with open(file_path, 'rb') as file:\n",
    "        file_count = 0\n",
    "        current_size = 0\n",
    "        output_file = None\n",
    "        \n",
    "        for line in file:\n",
    "            if output_file is None or current_size >= chunk_size * 1024 * 1024:\n",
    "                if output_file:\n",
    "                    output_file.close()\n",
    "                file_count += 1\n",
    "                current_size = 0\n",
    "                file_name = f'lifebear_{str(file_count).zfill(3)}.csv'\n",
    "                current_file_path = os.path.join(output_directory, file_name)\n",
    "                output_file = open(current_file_path, 'w', encoding='utf-8')\n",
    "\n",
    "            try:\n",
    "                decoded_line = line.decode('utf-8')  # Try decoding with UTF-8\n",
    "            except UnicodeDecodeError:\n",
    "                decoded_line = line.decode('latin-1')  # If that fails, try decoding with Latin-1\n",
    "            \n",
    "            output_file.write(decoded_line)\n",
    "            current_size += len(line)\n",
    "\n",
    "        if output_file:\n",
    "            output_file.close()\n",
    "\n",
    "    print(f'Splitting complete. Total files created: {file_count}')\n",
    "\n",
    "# Usage\n",
    "split_file(file_path, 100, output_folder)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9492ceb9-02c2-4df4-b48e-b385a6aa8cb1",
   "metadata": {},
   "source": [
    "### Review of chunked dataset\n",
    "> Loading of chunked file into pandas. Note the separating character for this dataset is a semi-colon & not the traditional comma character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8673c38-6c03-4f6b-b31e-94288afbd664",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\".//datasets//ingested//lifebear//records//chunks//lifebear_001.csv\"\n",
    "\n",
    "lifebear_dataset = pd.read_csv(file_path, sep=';', low_memory=True)\n",
    "lifebear_dataset.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e089c3-d5a6-4ef2-b655-f01f45a36cc2",
   "metadata": {},
   "source": [
    "### Removal of duplicate records/rows based on specific columns\n",
    "> To ensure consistent data we check for the appearance of duplicate rows based on unique columns (example email, mobile, credit card number, etc)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33963ffc-c629-4a9d-9ab9-ead6100207dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\".//datasets//Condo//records//condo_raw_dataset.csv\"\n",
    "output_file_name = f\".//datasets//Condo//records//v0//condo_raw_dataset_wd.csv\"\n",
    "\n",
    "condo_dataset = pd.read_csv(file_path, low_memory=True)\n",
    "condo_wd_dataset = condo_dataset.drop_duplicates(subset=[\"first_name\", \"email\", \"work_phone\"])\n",
    "condo_wd_dataset.to_csv(output_file_name, encoding='utf-8', index=False)\n",
    "\n",
    "condo_dataset = pd.read_csv(output_file_name, low_memory=True)\n",
    "# condo_dataset.head(5)\n",
    "condo_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ee072d-58e6-4008-9c2e-f692bbd6ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "# Function to check if a string contains only letters\n",
    "def only_letters(text):\n",
    "    return text.isalpha()\n",
    "\n",
    "def only_numbers(phone):\n",
    "    phone_pattern = re.compile(r'^[0-9(). ]+$')\n",
    "    return bool(re.fullmatch(phone_pattern, str(phone)))\n",
    "\n",
    "def validate_characters(username):\n",
    "    name_pattern = re.compile(r'^[a-zA-Z0-9&|#()/\\-._ ]*$')\n",
    "    return bool(re.fullmatch(name_pattern, str(username)))\n",
    "\n",
    "def validate_length(username):\n",
    "    username_str = str(username)\n",
    "    return len(username_str) < 16 and len(username_str.encode('utf-8')) < 16\n",
    "\n",
    "def validate_username(username):\n",
    "    return validate_characters(username) and validate_length(username)\n",
    "\n",
    "def validate_name(name):\n",
    "    name_pattern = re.compile(r'^[a-zA-Z]+$')\n",
    "    return bool(re.fullmatch(name_pattern, str(name)))\n",
    "\n",
    "def validate_phone(phone):\n",
    "    phone_pattern = re.compile(r'^[0-9().+ ]+$')\n",
    "    return bool(re.fullmatch(phone_pattern, str(phone)))\n",
    "\n",
    "def validate_ip(ip):\n",
    "    ip_pattern = re.compile(r'^[0-9.:]+$')\n",
    "    return bool(re.fullmatch(ip_pattern, str(ip)))\n",
    "\n",
    "def validate_passhash(pass_hash):\n",
    "    state_pattern = re.compile(r'^[a-zA-Z0-9&|#$_()\\/.+= ]+$')\n",
    "    return bool(re.fullmatch(state_pattern, str(pass_hash)))\n",
    "\n",
    "def validate_dob(dob):\n",
    "    dob_pattern = re.compile(r'^[0-9|:()/\\- ]+$')\n",
    "    return bool(re.fullmatch(dob_pattern, str(dob)))\n",
    "\n",
    "def validate_email(email):\n",
    "    email_pattern = re.compile(r'^[a-z0-9]+[\\._]?[a-z0-9]+[@]\\w+[.]\\w+$')\n",
    "    return bool(re.fullmatch(email_pattern, str(email)))\n",
    "\n",
    "def validate_address(address):\n",
    "    address_pattern = re.compile(r'^[a-zA-Z0-9&|#$()\\/.+=: ]+$')\n",
    "    return bool(re.fullmatch(address_pattern, str(address)))\n",
    "\n",
    "def validate_state(state):\n",
    "    state_pattern = re.compile(r'^[a-zA-Z&|#(). ]+$')\n",
    "    return bool(re.fullmatch(state_pattern, str(state)))\n",
    "\n",
    "\n",
    "def to_camel_case(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    return ' '.join(word.capitalize() for word in s.split())\n",
    "\n",
    "\n",
    "def to_lower(s):\n",
    "    if s is None:\n",
    "        return None\n",
    "    return s.lower()\n",
    "\n",
    "\n",
    "def convert_date_format(date_str):\n",
    "    # Parse the input date string\n",
    "    input_date = datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    # Convert the date to the desired format\n",
    "    output_date_str = input_date.strftime(\"%m/%d/%Y\")\n",
    "    return output_date_str\n",
    "\n",
    "# Function to process a single file\n",
    "def process_file(input_file_path, output_folder):\n",
    "    filename = os.path.splitext(os.path.basename(input_file_path))[0]\n",
    "    output_file_path = os.path.join(output_folder, f\"{filename}_structured.csv\")\n",
    "    garbage_file_path = os.path.join(output_folder, f\"garbage//{filename}_garbage.csv\")\n",
    "\n",
    "    # Create the output and garbage files with UTF-8 encoding\n",
    "    with open(output_file_path, 'w', newline='', encoding='utf8') as output_file, open(garbage_file_path, 'w', newline='', encoding='utf8') as garbage_file:\n",
    "        output_writer = csv.writer(output_file)\n",
    "        garbage_writer = csv.writer(garbage_file)\n",
    "        # Write the header row to the output file\n",
    "        \n",
    "        # id[0];username[1];email[2];password[3];salt[5];birthday_on[6];gender[7]\n",
    "        header = [\"username\", \"email\", \"password\", \"salt\", \"dob\", \"gender\"]\n",
    "        output_writer.writerow(header)\n",
    "\n",
    "        # Open the input file for reading and process the contents with UTF-8 encoding\n",
    "        with open(input_file_path, 'r', encoding='utf8') as input_file:\n",
    "            # Skip the first line (header definitions)\n",
    "            # next(input_file)\n",
    "\n",
    "            # Process the remaining lines\n",
    "            for line in input_file:\n",
    "                # Initialize variables with None\n",
    "                username = email = password = dob = salt = gender = None\n",
    "                # Split the line using \";\" as the separator\n",
    "                parts = line.strip().split(';')\n",
    "\n",
    "\n",
    "                \n",
    "                 # Validate USERNAME record | USERNAME[1]\n",
    "                if len(parts) > 1 and parts[1]:\n",
    "                    try:\n",
    "                        email_pattern = r'\\S+@\\S+'\n",
    "                        if validate_username(parts[1].replace('\"', '').replace(\"'\", '').replace(' ', '')) and not re.match(email_pattern, parts[1].replace('\"', '').replace(\"'\", '').replace(' ', '')):\n",
    "                            username = parts[1].replace('.', '').replace('\"', '').replace(\"'\", '')\n",
    "                    except ValueError:\n",
    "                        # Write the invalid record to the garbage file\n",
    "                        garbage_writer.writerow([line.strip()])\n",
    "                        continue\n",
    "                else:\n",
    "                    username = None\n",
    "\n",
    "                \n",
    "                # Validate EMAIL record | EMAIL_ID[2]\n",
    "                if len(parts) > 2:\n",
    "                    raw_email = parts[2]\n",
    "                    \n",
    "                    # Clean the email\n",
    "                    cleaned_email = re.sub(r\"[\\\"'\\t#]\", '', raw_email).strip()\n",
    "                    \n",
    "                    if cleaned_email.lower() == \"null\" or cleaned_email == \"\":\n",
    "                        email = None\n",
    "                    else:\n",
    "                        email_pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "                        if re.match(email_pattern, cleaned_email):\n",
    "                            email = cleaned_email\n",
    "                        else:\n",
    "                            # Write the invalid record to the garbage file\n",
    "                            garbage_writer.writerow([line.strip()])\n",
    "                            continue\n",
    "                else:\n",
    "                    email = None\n",
    "\n",
    "                # id[0];username[1];email[2];password[3];salt[5];birthday_on[6];gender[7]\n",
    "                \n",
    "                # Validate DOB record | DOB[6]\n",
    "                if len(parts) > 6 and parts[6]:\n",
    "                    try:\n",
    "                        email_pattern = r'\\S+@\\S+'\n",
    "                        if not re.match(email_pattern, parts[6].replace('\"', '').replace(\"'\", '').replace('.', '').replace('-', '')):\n",
    "                            dob = convert_date_format(parts[6].replace('\"', '').replace(\"'\", '').replace(\"+\", '').replace('\\t', '').replace(' ', ''))\n",
    "                    except ValueError:\n",
    "                        # Write the invalid record to the garbage file\n",
    "                        garbage_writer.writerow([line.strip()])\n",
    "                        continue\n",
    "                else:\n",
    "                    dob = None\n",
    "\n",
    "                \n",
    "                  # Test GENDER record | GENDER [7]\n",
    "                if len(parts) > 7 and parts[7]:\n",
    "                    try:\n",
    "                        email_pattern = r'\\S+@\\S+'\n",
    "                        if only_numbers(parts[7].replace(\"'\", \"\").replace('\"', '').replace(\"-\", \"\").replace(\"+\", \"\").replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\")) and not re.match(email_pattern, parts[7].replace('\"', '').replace(\"+\", \"\").replace(\"'\", '').replace(' ', '').replace('-', '')):\n",
    "                            gender = parts[7].replace('\"', '').replace(\"'\", '').replace(\"+\", '').replace('~', '').replace('\\t', '').replace(\" \", \"\").replace(\"(\", \"\").replace(\")\", \"\").replace(\"-\", \"\").replace(\"+\", \"\")\n",
    "                    except ValueError:\n",
    "                        # Write the invalid record to the garbage file\n",
    "                        garbage_writer.writerow([line.strip()])\n",
    "                        continue\n",
    "                else:\n",
    "                    gender = None\n",
    "\n",
    "\n",
    "                  # Test PASSWORD record | PASSWORD [3]\n",
    "                if len(parts) > 3 and parts[3]:\n",
    "                    try:\n",
    "                        email_pattern = r'\\S+@\\S+'\n",
    "                        if validate_address(parts[3].replace(\"'\", \"\").replace('\"', '').replace(\"-\", \"\").replace(\" \", \"\")) and not re.match(email_pattern, parts[3].replace('\"', '').replace(\"'\", '').replace(' ', '').replace('-', '')):\n",
    "                            password = parts[3].replace('\"', '').replace(\"'\", '').replace(\"+\", '').replace('~', '').replace('\\t', '').replace(',', '')\n",
    "                    except ValueError:\n",
    "                        # Write the invalid record to the garbage file\n",
    "                        garbage_writer.writerow([line.strip()])\n",
    "                        continue\n",
    "                else:\n",
    "                    password = None\n",
    "\n",
    "\n",
    "                  # Validate SALT record | SALT[5]\n",
    "                if len(parts) > 5 and parts[5]:\n",
    "                    try:\n",
    "                        email_pattern = r'\\S+@\\S+'\n",
    "                        if not re.match(email_pattern, parts[5].replace('\"', '').replace(\"'\", '').replace('.', '')):\n",
    "                            salt = parts[5].replace('\"', '').replace(\"'\", '').replace(\"+\", '').replace('/', '').replace('\\t', '')\n",
    "                    except ValueError:\n",
    "                        # Write the invalid record to the garbage file\n",
    "                        garbage_writer.writerow([line.strip()])\n",
    "                        continue\n",
    "                else:\n",
    "                    salt = None\n",
    "\n",
    "                \n",
    "                # firstname = to_camel_case(firstname)\n",
    "                # lastname = to_camel_case(lastname)\n",
    "                # address = to_camel_case(address)\n",
    "                # city = to_camel_case(city)\n",
    "                email = to_lower(email)\n",
    "                username = to_lower(username)\n",
    "\n",
    "                # first_name,last_name,email,mobile_phone,work_phone\n",
    "                # Write the cleaned data as a CSV line to the output file\n",
    "                output_writer.writerow([username, email, password, salt, dob, gender])\n",
    "\n",
    "            print(f\"Conversion completed for {filename}. Valid data saved to {filename}_structured.csv. Invalid data saved to {filename}_garbage.csv.\")\n",
    "\n",
    "# Specify the folder containing the CSV files\n",
    "input_folder = f\"./datasets/ingested/Lifebear/records/chunks/\"\n",
    "# Specify the folder where the output files will be saved\n",
    "output_folder = f\"./datasets/ingested/Lifebear/processed/\"\n",
    "# List all CSV files in the input folder\n",
    "csv_files = [file for file in os.listdir(input_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "# Process each CSV file in the folder\n",
    "for csv_file in csv_files:\n",
    "    input_file_path = os.path.join(input_folder, csv_file)\n",
    "    process_file(input_file_path, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a15e29a6-54f6-4195-b281-c1a9cd430e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def remove_empty_lines(input_file, output_file):\n",
    "    try:\n",
    "        with open(input_file, 'r', encoding='utf-8') as infile, open(output_file, 'w', encoding='utf-8') as outfile:\n",
    "            for line in infile:\n",
    "                # Strip leading/trailing whitespace and check if line is not empty\n",
    "                if line.strip() and ',,,,' not in line and 'NULL,,,NULL,' not in line:\n",
    "                    outfile.write(line)\n",
    "        \n",
    "        print(f\"Lines with specified text removed successfully from {input_file}!\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred with file {input_file}: {e}\")\n",
    "\n",
    "\n",
    "# Specify the folder containing the CSV files\n",
    "input_folder = \"./datasets/ingested/Lifebear/processed/\"\n",
    "# Specify the folder where the output files will be saved\n",
    "output_folder = \"./datasets/ingested/Lifebear/to-be-ingested/\"\n",
    "# List all CSV files in the input folder\n",
    "csv_files = [file for file in os.listdir(input_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "# Process each CSV file in the folder\n",
    "for csv_file in csv_files:\n",
    "    input_file_path = os.path.join(input_folder, csv_file)\n",
    "    output_file_path = os.path.join(output_folder, csv_file)  # Ensure the output file path includes the original file name\n",
    "    remove_empty_lines(input_file_path, output_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11bea113-61d9-45ab-8343-24919da115b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import codecs\n",
    "\n",
    "# Function to remove non-UTF-8 encoded characters from a string\n",
    "def remove_non_utf8(text):\n",
    "    return text.encode('utf-8', 'ignore').decode('utf-8')\n",
    "\n",
    "# Specify the folder containing the CSV files\n",
    "input_folder = \".//datasets//ingested//Lifebear//to-be-ingested//\"\n",
    "\n",
    "# Specify the folder where the cleaned files will be saved\n",
    "output_folder = \".//datasets//ingested//Lifebear//to-be-ingested//final//\"\n",
    "\n",
    "# List all CSV files in the input folder\n",
    "csv_files = [file for file in os.listdir(input_folder) if file.endswith(\".csv\")]\n",
    "\n",
    "# Process each CSV file in the folder\n",
    "for csv_file in csv_files:\n",
    "    input_file_path = os.path.join(input_folder, csv_file)\n",
    "    output_file_path = os.path.join(output_folder, f\"{os.path.splitext(csv_file)[0]}_cleaned.csv\")\n",
    "\n",
    "    # Open the input CSV file for reading and the output CSV file for writing\n",
    "    with codecs.open(input_file_path, 'r', encoding='utf-8', errors='ignore') as input_file, \\\n",
    "            open(output_file_path, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        \n",
    "        # Initialize CSV reader and writer\n",
    "        csv_reader = csv.reader(input_file)\n",
    "        csv_writer = csv.writer(output_file)\n",
    "\n",
    "        # Process each row in the input CSV file\n",
    "        for row in csv_reader:\n",
    "            # Remove non-UTF-8 encoded characters from each field in the row\n",
    "            cleaned_row = [remove_non_utf8(field) for field in row]\n",
    "            # Write the cleaned row to the output CSV file\n",
    "            csv_writer.writerow(cleaned_row)\n",
    "\n",
    "    print(f\"Cleaning completed for {csv_file}. Cleaned data saved to {os.path.basename(output_file_path)}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d58227-8f89-4587-97b1-225823df4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the path to the folder containing the files\n",
    "# folder_path = f\".//datasets//Sport2000//to-be-ingested//final//\"\n",
    "folder_path = f\".//datasets//ingested//Lifebear//to-be-ingested//final//\"\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Construct the full path to the file\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Read lines, remove specific characters, and update the original file\n",
    "        with open(file_path, 'r+', encoding='latin-1') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "            # Remove specific characters from each line\n",
    "            cleaned_lines = [line.replace('*', '').replace('------', '').replace('?', '').replace('~', '').replace('^', '').replace('$', '').replace('\"', '').replace(\"'\", '').replace('|', '').replace('!', '').replace('#', '').replace('(', '').replace(')', '').replace('\\\\', '').replace('/@', '@').replace('zz', '').replace('n/a', '').replace('`', '').replace('&', '').replace('*', '').replace('+', '')              # Add more characters to remove\n",
    "                             for line in lines]\n",
    "\n",
    "            # Move the file pointer to the beginning and truncate the file\n",
    "            file.seek(0)\n",
    "            file.truncate()\n",
    "\n",
    "            # Write the cleaned lines back to the file\n",
    "            file.writelines(cleaned_lines)\n",
    "\n",
    "        print(f\"Specific characters removed from the file: {filename}\")\n",
    "\n",
    "print(\"Cleaning operation completed for all files in the folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0333f54b-6716-4b1f-a1e7-d8b365458e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\".//datasets//ingested//Lifebear//to-be-ingested//final//lifebear_004_structured_cleaned.csv\"\n",
    "output_file_name = f\".//datasets//ingested//Lifebear//to-be-ingested//final//lifebear_wd_final_4.csv\"\n",
    "f\".\"\n",
    "condo_dataset = pd.read_csv(file_path, low_memory=True)\n",
    "condo_wd_dataset = condo_dataset.drop_duplicates(subset=[\"username\", \"salt\", \"email\"])\n",
    "condo_wd_dataset.to_csv(output_file_name, encoding='utf-8', index=False)\n",
    "\n",
    "condo_dataset = pd.read_csv(output_file_name, low_memory=True)\n",
    "# condo_dataset.head(5)\n",
    "condo_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7a68c7-0d2b-4079-82eb-4d34a1b06e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify the path to the folder containing the files\n",
    "folder_path = f\".//datasets//ingested//Lifebear//to-be-ingested//final//v0//\"\n",
    "\n",
    "# Function to truncate a string to a maximum of 32 bytes\n",
    "def truncate_field(field, max_bytes=64):\n",
    "    byte_field = field.encode('utf-8')  # Encode the field in UTF-8\n",
    "    if len(byte_field) > max_bytes:\n",
    "        truncated_field = byte_field[:max_bytes].decode('utf-8', errors='ignore')  # Truncate and ignore partial characters\n",
    "        return truncated_field\n",
    "    return field\n",
    "\n",
    "# Function to clean and truncate fields in a CSV row\n",
    "def clean_and_truncate_row(row, columns_to_truncate, max_bytes=32):\n",
    "    # Clean and truncate each field in the specified columns\n",
    "    cleaned_row = [\n",
    "        truncate_field(\n",
    "            field.replace('*', '').replace('..', '').replace('------', '').replace('?', '')\n",
    "                 .replace('~', '').replace('^', '').replace('$', '').replace('\"', '')\n",
    "                 .replace(\"'\", '').replace('|', '').replace('!', '').replace('#', '')\n",
    "                 .replace('(', '').replace(')', '').replace('\\\\', '').replace('/@', '@')\n",
    "                 .replace('zz', '').replace('n/a', '').replace('`', '').replace('&', '')\n",
    "                 .replace('*', '').replace('+', '').replace('null', '').replace('.0', ''),  # Additional characters to remove\n",
    "            max_bytes if i in columns_to_truncate else None\n",
    "        ) if i in columns_to_truncate else field\n",
    "        for i, field in enumerate(row)\n",
    "    ]\n",
    "    return cleaned_row\n",
    "\n",
    "# Iterate over each file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Construct the full path to the file\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "        # Read and process the CSV file using UTF-8 encoding\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file)\n",
    "            lines = list(reader)\n",
    "\n",
    "            # Specify which columns to truncate (for example: 0=firstname, 1=lastname, etc.)\n",
    "            columns_to_truncate = [0,1,5]  # Modify this list to target the right columns\n",
    "            \n",
    "            # Clean and truncate each line\n",
    "            cleaned_lines = [clean_and_truncate_row(line, columns_to_truncate) for line in lines]\n",
    "\n",
    "        # Write the cleaned and truncated lines back to the file using UTF-8 encoding\n",
    "        with open(file_path, 'w', encoding='utf-8', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerows(cleaned_lines)\n",
    "\n",
    "        print(f\"Cleaning and truncation completed for the file: {filename}\")\n",
    "\n",
    "print(\"Operation completed for all files in the folder.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e9a048-4d96-419f-85e0-d6c07e1394d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_dob(name_series):\n",
    "    name_errors = []\n",
    "    #    name_pattern = re.compile(r'^[a-zA-Z&|#(). ]+$')\n",
    "    # Pattern to check if the name contains only letters\n",
    "    name_pattern = re.compile(r'^[0-9&|#()-/. ]+$')\n",
    "\n",
    "    for i, name in enumerate(name_series):\n",
    "        if not re.match(name_pattern, str(name)):\n",
    "            name_errors.append(f\"Invalid characters in name in row {i + 1}: {name}\")\n",
    "\n",
    "    return name_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae08bc43-7483-4d37-9c0c-f361a9dfb2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_usernames(name_series):\n",
    "    name_errors = []\n",
    "    #    name_pattern = re.compile(r'^[a-zA-Z&|#(). ]+$')\n",
    "    # Pattern to check if the name contains only letters\n",
    "    name_pattern = re.compile(r'^[a-zA-Z0-9&|#(). ]+$')\n",
    "\n",
    "    for i, name in enumerate(name_series):\n",
    "        if not re.match(name_pattern, str(name)):\n",
    "            name_errors.append(f\"Invalid characters in name in row {i + 1}: {name}\")\n",
    "\n",
    "    return name_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a9a477-b600-4154-9223-354562b82dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_names(name_series):\n",
    "    name_errors = []\n",
    "    #    name_pattern = re.compile(r'^[a-zA-Z&|#(). ]+$')\n",
    "    # Pattern to check if the name contains only letters\n",
    "    name_pattern = re.compile(r'^[a-zA-Z]+$')\n",
    "\n",
    "    for i, name in enumerate(name_series):\n",
    "        if not re.match(name_pattern, str(name)):\n",
    "            name_errors.append(f\"Invalid characters in name in row {i + 1}: {name}\")\n",
    "\n",
    "    return name_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab40dbd-4b0c-4d82-b415-cb90e3e36f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_emails(email_series):\n",
    "    email_errors = []\n",
    "\n",
    "   # Improved email pattern\n",
    "    email_pattern = re.compile(r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$')\n",
    "\n",
    "    for i, email in enumerate(email_series):\n",
    "       # Check if the email is blank\n",
    "        if pd.isnull(email) or str(email).strip() == '':\n",
    "            continue  # Skip validation for blank emails\n",
    "\n",
    "        # Check if the email matches the pattern\n",
    "        if not re.match(email_pattern, str(email)):\n",
    "            email_errors.append(f\"Invalid email address in row {i + 1}: {email}\")\n",
    "\n",
    "    return email_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbaa9cf-5d2d-4822-8551-f2c559ebc595",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_report(validation_errors, total_rows):\n",
    "    if validation_errors:\n",
    "        # Plot a bar chart of the number of errors per column\n",
    "        columns = list(validation_errors.keys())\n",
    "        error_count = [len(error) for error in validation_errors.values()]\n",
    "\n",
    "        # Calculate the percentage of errors\n",
    "        error_percentage = [(count / total_rows) * 100 for count in error_count]\n",
    "         # Manually set the size of the figure\n",
    "        plt.figure(figsize=(10, 6))\n",
    "\n",
    "        plt.bar(columns, error_count)\n",
    "        plt.xlabel('Columns')\n",
    "        plt.ylabel('Error Count')\n",
    "        plt.title('Lifebear Dataset - Record Validation Testing')\n",
    "        \n",
    "        # Add error count labels on the bars\n",
    "        # for i, count in enumerate(error_count): \n",
    "         # Add error percentage labels on the bars\n",
    "        for i, percentage in enumerate(error_percentage):\n",
    "            # plt.text(i, count + 0.1, str(count), ha='center', va='bottom')\n",
    "            plt.text(i, percentage + 0.1, f\"{percentage:.2f}%\", ha='center', va='bottom')\n",
    "\n",
    "        # Automatically adjust layout to prevent overlapping\n",
    "       # plt.tight_layout()\n",
    "\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Data type validation successful!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78ca9bf-538d-4fef-9b90-16fcb5956b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\".//datasets//ingested//Lifebear//to-be-ingested//final//v0//lifebear_wd_final_3.csv\"\n",
    "df = pd.read_csv(file_path, low_memory=False)\n",
    "pd.options.display.min_rows = 10\n",
    "\n",
    "# dob_errors = validate_dob(df[\"dob\"])\n",
    "email_errors = validate_emails(df[\"email\"])\n",
    "username_errors = validate_usernames(df[\"username\"])\n",
    "\n",
    "# Combine all errors into one dictionary\n",
    "all_errors = {\n",
    "    'Email Errors': email_errors,\n",
    "    'Username Errors': username_errors\n",
    "}\n",
    "\n",
    "# Using the shape attribute\n",
    "num_rows, num_columns = df.shape\n",
    "# total_errors = len(email_errors) + len(state_errors) + len(zip_errors) + len(country_errors) + len(city_errors)\n",
    "total_errors = len(email_errors) + len(username_errors)\n",
    "generate_report(all_errors, num_rows)\n",
    "\n",
    "print(f'Verification Testing Complete. \\n\\n Total {num_rows} Rows Tested, {total_errors} Invalid Records FOUND!')\n",
    "# Last two Email errors\n",
    "print(f'Email Errors {email_errors[-10:]}')\n",
    "print()\n",
    "# Last two DOB errors\n",
    "\n",
    "print(f'Firstname Code Errors {username_errors[-12:]}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bb9e947-0097-49d8-8f04-8453bb815a1e",
   "metadata": {},
   "source": [
    "## Uploading S3 records to RedShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3591991-fdc6-47bd-92c4-faab24a833dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=os.getenv('REDSHIFT_STAGE_HOST'),\n",
    "    port=os.getenv('REDSHIFT_STAGE_PORT'),\n",
    "    database=os.getenv('REDSHIFT_STAGE_DB'),\n",
    "    user=os.getenv('REDSHIFT_STAGE_USER'),\n",
    "    password=os.getenv('REDSHIFT_STAGE_PASSWORD')\n",
    ")\n",
    "\n",
    "# username, email, password, salt, dob, gender\n",
    "cursor = conn.cursor()\n",
    "create_table_stmt = f\"\"\"\n",
    "CREATE TABLE \"dev\".\"lifebear_user_data\"(\n",
    "\"username\" character varying(4096) NULL, \n",
    "\"email\" character varying(4096) NULL, \n",
    "\"password\" character varying(4096) NULL, \n",
    "\"salt\" character varying(4096) NULL, \n",
    "\"dob\" character varying(4096) NULL, \n",
    "\"gender\" character varying(4096) NULL\n",
    ")ENCODE AUTO;\n",
    "\"\"\"\n",
    "\n",
    "cursor.execute(create_table_stmt)\n",
    "conn.commit()\n",
    "\n",
    "info_table_name = \"leaks.prod.info\"\n",
    "leak_info = \"In early 2019, the Japanese schedule app Lifebear appeared for sale on a dark web marketplace amongst a raft of other hacked websites. The breach exposed almost 3.7M unique email addresses, usernames and passwords stored as salted MD5 hashes.\"\n",
    "leak_source = \"lifebear.com\"\n",
    "leak_download = \"https://pixeldrain.com/u/RDKXZu9y\"\n",
    "leak_date = \"2019-02-28\"\n",
    "ingestion_date = \"2024-10-04\"\n",
    "\n",
    "info_insert_stmt = f'''\n",
    "INSERT INTO {info_table_name}\n",
    "(\"information\", \"source\", \"foundwhere\", \"leakdate\", \"adddate\")\n",
    "VALUES ('{leak_info}', '{leak_source}', '{leak_download}', '{leak_date}', '{ingestion_date}');\n",
    "'''\n",
    "\n",
    "cursor.execute(info_insert_stmt)\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "\n",
    "info_v2_query_stmt = 'select * from leaks.prod.info order by id desc LIMIT 10'\n",
    "cursor.execute(info_v2_query_stmt)\n",
    "conn.commit()\n",
    "\n",
    "# cursor.execute(diag_query_str)\n",
    "\n",
    "# # # cursor.fetchone()\n",
    "# cursor.fetchall()\n",
    "for rec in cursor:\n",
    "    print(rec)\n",
    "\n",
    "cursor.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1dda43-ad0f-4c65-8918-ada098be6652",
   "metadata": {},
   "source": [
    "## Uploading S3 records to RedShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a02fcb-ced1-471a-b847-cfb26016876c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "load_dotenv()\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=os.getenv('REDSHIFT_STAGE_HOST'),\n",
    "    port=os.getenv('REDSHIFT_STAGE_PORT'),\n",
    "    database=os.getenv('REDSHIFT_STAGE_DB'),\n",
    "    user=os.getenv('REDSHIFT_STAGE_USER'),\n",
    "    password=os.getenv('REDSHIFT_STAGE_PASSWORD')\n",
    ")\n",
    "\n",
    "   \n",
    "cursor = conn.cursor()\n",
    "table_name = \"leaks.dev.lifebear_user_data\"\n",
    "s3_resource = \"s3://leaksraw/priorities_1_NOTDONE/Lifebear/to-be-ingested/final/v0/\"\n",
    "\n",
    "# username,email,password,salt,dob,gender\n",
    "copy_stmt = f'''\n",
    "COPY {table_name} (\"username\", \"email\", \"password\", \"salt\", \"dob\", \"gender\")\n",
    "FROM '{s3_resource}' IAM_ROLE 'arn:aws:iam::882736527955:role/service-role/AmazonRedshift-CommandsAccessRole-20230606T095354' \n",
    "DELIMITER ','\n",
    "REMOVEQUOTES\n",
    "ESCAPE\n",
    "IGNOREHEADER 1\n",
    "REGION AS 'us-east-1';\n",
    "'''\n",
    "\n",
    "cursor.execute(copy_stmt)\n",
    "conn.commit()\n",
    "\n",
    "query_str = 'SELECT count(*) FROM leaks.dev.lifebear_user_data'\n",
    "cursor.execute(query_str)\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "for rec in cursor:\n",
    "    print(rec)\n",
    "# 3827910\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5aa4da2-a276-415a-b8f7-ba18953a9095",
   "metadata": {},
   "source": [
    "# Confirmation Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b0615b-cd54-4b05-a62c-14cc47e6ff24",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=os.getenv('REDSHIFT_STAGE_HOST'),\n",
    "    port=os.getenv('REDSHIFT_STAGE_PORT'),\n",
    "    database=os.getenv('REDSHIFT_STAGE_DB'),\n",
    "    user=os.getenv('REDSHIFT_STAGE_USER'),\n",
    "    password=os.getenv('REDSHIFT_STAGE_PASSWORD')\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "# query_str = 'SELECT count(*) FROM leaks.fling_user_data'\n",
    "query_str = 'SELECT * FROM dev.lifebear_user_data LIMIT 20'\n",
    "# query_str = 'SELECT count(*) FROM dev.s2000_user_data'\n",
    "\n",
    "cursor.execute(query_str)\n",
    "conn.commit()\n",
    "\n",
    "for rec in cursor:\n",
    "    print(rec)\n",
    "\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "896be573-097f-4918-b558-758bfbfbd4b1",
   "metadata": {},
   "source": [
    "# Final Ingestion to Leaks Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a2ddf2-d27f-41ee-badd-b3d84c0ca0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=os.getenv('REDSHIFT_STAGE_HOST'),\n",
    "    port=os.getenv('REDSHIFT_STAGE_PORT'),\n",
    "    database=os.getenv('REDSHIFT_STAGE_DB'),\n",
    "    user=os.getenv('REDSHIFT_STAGE_USER'),\n",
    "    password=os.getenv('REDSHIFT_STAGE_PASSWORD')\n",
    ")\n",
    "# name,email,dob,mobile,national_id\n",
    "# LEAKS ==> \"username\", \"firstname\", \"lastname\", \"email\", \"password\", \"ip\", \"address\" \"company_name\", \"phone\",  \"dob\", \"web_domain\"\n",
    "# Lifebear ==> username,email,password,salt,dob,gender\n",
    "\n",
    "cursor = conn.cursor()\n",
    "\n",
    "leaks_table_name = \"leaks.prod.leaks\"\n",
    "ingested_table_name = \"leaks.dev.lifebear_user_data\"\n",
    "\n",
    "leaks_insert_stmt = f'''\n",
    "INSERT INTO {leaks_table_name}\n",
    "(\"username\", \"email\", \"password\", \"dob\", \"info_id\") \n",
    "SELECT \"username\", \"email\",\"password\", \"dob\", 4519 AS \"info_id\"\n",
    "FROM {ingested_table_name}\n",
    "'''\n",
    "cursor.execute(leaks_insert_stmt)\n",
    "conn.commit()\n",
    "\n",
    "leaks_count_stmt = 'select count(*) from leaks.prod.leaks'\n",
    "cursor.execute(leaks_count_stmt)\n",
    "count = cursor.fetchone()[0]\n",
    "\n",
    "\n",
    "print(f'{count} Records successfully ingested!')\n",
    "\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d8bf85-c2af-4f08-8e39-d9697c7197c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv()\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=os.getenv('REDSHIFT_STAGE_HOST'),\n",
    "    port=os.getenv('REDSHIFT_STAGE_PORT'),\n",
    "    database=os.getenv('REDSHIFT_STAGE_DB'),\n",
    "    user=os.getenv('REDSHIFT_STAGE_USER'),\n",
    "    password=os.getenv('REDSHIFT_STAGE_PASSWORD')\n",
    ")\n",
    "\n",
    "cursor = conn.cursor()\n",
    "query_str = 'SELECT * FROM leaks.prod.leaks where info_id = 4519 LIMIT 10'\n",
    "# query_str = 'SELECT count(*) FROM leaks.prod.leaks where info_id = 4054'\n",
    "cursor.execute(query_str)\n",
    "conn.commit()\n",
    "\n",
    "\n",
    "for rec in cursor:\n",
    "    print(rec)\n",
    "\n",
    "cursor.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12675471-93c0-47da-9d26-9b12d6f53db1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
